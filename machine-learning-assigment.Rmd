---
title: "Practical Machine Learning Assignment"
output: html_document
--- 

#Introduction

```{r, echo=FALSE}
library(caret)
library(dplyr)
```

With increasing levels of obesity in the western world today we are all encouraged to lead better lifestyles, eating a healthier diet and doing more exercise. It is very easy for an individual to qunatify how much of an activity they have done but they have not necessarily done it in an effective manner to get the maximum benefit from it. Devices can be used to collect large amounts of data for an individual quantifying how well they are doing a particular activity.

For this project I will be using data( http://groupware.les.inf.puc-rio.br/har.)
from accelerometers on the belt, forearm, arm and dumbbell of 6 participants. (The goal is to predict the manner in which they did the exercise, represented by the field classe in the training data.

The training data is composed of 19,622 rows and 160 fields. A lot of the fields in the training data are largely composed of nulls or empty character strings. In the testing data these fields are completely null- they cannot be used for prediction purposes so they should not be included in building the model.

```{r}
pml.training <- read.csv("C:/Users/Deborah/Desktop/practical_machine_learning/pml-training.csv")
data <- pml.training[ , ! apply(pml.training , 2 , function(x) any(is.na(x)))]
training_data <- data[ , !apply(data, 2, function(x) any(x==""))]
```

#Pre-processing

If a data set has a large number of potential predictors then it should be taken into consideration that not all may be required. Predictors which are highly correlated will likely have the same effect in predicting the class, therefore there is no need to include them both. Analysis of the correlation in the numeric fields in the data revealed that many pairs were highly correlated (correlation greater than 0.8). Because of this I am going to use principal component analysis in buildng my model to reduce the dimensionality of the data set and the likelihood of overfitting the model. Prior to this I have used the preProcess function from caret to determine how many of the variables would be required to capture 95% of the variance- just 25.

```{r}
M <- abs(cor(training_data[,8:59]))
diag(M) <- 0
which(M > 0.8, arr.ind = T)

preProc <- preProcess(training_data[, 8:59], method = "pca")
preProc
```

#Which model to use?
The outcome variable that I want to predict is a categorical variable of muliple classes, so I would like to use a tree-based model. I will be comparing the accuracy and out-of-sample error between a classification tree, random forest and gradient boosting model to see which performs best. Random forests and gradient boosting will provide more accurate results than a standard classification tree, so should be reflected here. The main con of these methods aside from the accuracy gain is that they can lead to overfitting, so it is important to use cross-validation when building the model.


#Using cross validation in building the model

It is important that the test set is not used in building the model, otherwise it becomes part of the training set. I have used the createDataPartition function in caret to spilt the supplied training data into a separate training set and my own testing set (80% training to 20% testing). If a model is built soley upon one set of data , the temptation will always remain to keep overfitting the model on this data until it predicts on that data set with 100% accuracy. The issue with this is that the outcome variable in this data set is composed of signal and noise- we only want to predict the true siganl but an overfitted model will predict the noise too. As a result when the model is applied to a new data set it may not predict as accurately as it did in for the training set. In this function random samples are taken from within each class to preserve the overall class distribution of the data.

```{r}
set.seed(12345)
inTrain <- createDataPartition(y = training_data$classe, p = 0.8, list = FALSE)
training <- training_data[inTrain,]
testing <- training_data[-inTrain,]
```

#Fitting the Model
The first model I fitted was a tree model, with pre-processing using principal component analysis and cross validation using the bootstrap method. The tree model has a relatively low accuracy of just 0.2845 on my own testing set and an out of sample error of 2008.475.

```{r}
modelFit <- train(training$classe ~., method='rpart', preProcess = "pca", data = training)
confusionMatrix(testing$classe, predict(modelFit, testing))
compare_predictions <- data.frame(original=testing$classe, new_prediction=predict(modelFit, testing))
compare_predictions$class_prediction <- ifelse(compare_predictions$original == compare_predictions$new_prediction, "Correct", "Wrong")
wrong_predictions <- nrow(filter(compare_predictions, class_prediction == "Wrong"))
out_of_sample_error <- 1/(nrow(compare_predictions))*(wrong_predictions^2)
out_of_sample_error
```



The next model I have tried is a gradient boosting model, using 10-fold cross validation.This model achieved an accuracy of 0.9324 on my own testing data and an out of sample error of 17.9. I then fitted a random forest model to see if the out of sample error could be imporved further. I used bootstrapping as the method of cross validation for this model. It achieved an accuracy of 0.983 on my own testing data and an out of sample error rate of 0.45. I compared this with the results from random forest models using 10 fold and boot632 cross validation, which also obtained out of sample error rates between 0.4 and 0.5.

```{r}
modelFit <- train(training$classe ~., method='rf', preProcess = "pca", trControl = trainControl(method = "cv"), data = training)
confusionMatrix(testing$classe, predict(modelFit, testing))
compare_predictions <- data.frame(original=testing$classe, new_prediction=predict(modelFit, testing))
compare_predictions$class_prediction <- ifelse(compare_predictions$original == compare_predictions$new_prediction, "Correct", "Wrong")
wrong_predictions <- nrow(filter(compare_predictions, class_prediction == "Wrong"))
out_of_sample_error <- 1/(nrow(compare_predictions))*(wrong_predictions^2)
out_of_sample_error
```


Using this model on the test data, the following predictions were made for the classe variable:

```{r}
pml.testing <- read.csv("C:/Users/Deborah/Desktop/practical_machine_learning/pml-testing.csv")
testing_data <- pml.testing[ , ! apply(pml.testing , 2 , function(x) any(is.na(x)))]
predictions <- predict(modelFit, testing_data)
predictions
``` 
