---
title: "Practical Machine Learning Assignment"
output: html_document
--- 

#Introduction

With increasing levels of obesity in the western world today we are all encouraged to lead better lifestyles, eating a healthier diet and doing more exercise. It is very easy for an individual to qunatify how much of an activity they have done but they have not necessarily done it in an effective manner to get the maximum benefit from it. Devices can be used to collect large amounts of data for an individual quantifying how well they are doing a particular activity.

For this project I will be using data from accelerometers on the belt, forearm, arm and dumbbell of 6 participants. The goal is to predict the manner in which they did the exercise, represented by the field classe in the training data.

The training data is composed of 19,622 rows and 160 fields. A lot of the fields in the training data are largely composed of nulls or empty character strings. In the testing data these fields are completely null- they cannot be used for prediction purposes so they should not be included in building the model.




```{r}
library(caret)
pml.training <- read.csv("C:/Users/Deborah/Desktop/practical_machine_learning/pml-training.csv")
data <- pml.training[ , ! apply(pml.training , 2 , function(x) any(is.na(x)))]
training_data <- data[ , !apply(data, 2, function(x) any(x==""))]
```

#Pre-processing

If a data set has a large number of potential predictors then it should be taken into consideration that not all may be required. Predictors which are highly correlated will likely have the same effect in predicting the class, therefore there is no need to include them both. Analysis of the correlation in the numeric fields in the data revealed that many pairs were highly correlated (correlation greater than 0.8). Because of this I am going to use principal component analysis in buildng my model to reduce the dimensionality of the data set and the likelihood of overfitting the model. Prior to this I have used the preProcess function from caret to determine how many of the variables would be required to capture 95% of the variance- just 25.

```{r}
M <- abs(cor(training_data[,8:59]))
diag(M) <- 0
which(M > 0.8, arr.ind = T)

preProc <- preProcess(training_data[, 8:59], method = "pca")
preProc
```

#Which model to use?
Mention that the outcome is a categorical variable of multiple classes. what are the specific benefits of tree based methods? Why are random forests better than classi tree models?

#Using cross validation in building the model

It is important that the test set is not used in building the model, otherwise it becomes part of the training set. I have used the createDataPartition function in caret to spilt the supplied training data into separate training and testing sets (80% training to 20% testing). If a model is built soley upon one set of data , the temptation will always remain to keep overfitting the model on this data until it predicts on that data set with 100% accuracy. The issue with this is that the outcome variable in this data set is composed of signal and noise- we only want to predict the true siganl but an overfitted model will predict the noise too. As a result when the model is applied to a new data set it may not predict as accurately as it did in for the training set. In this function random samples are taken from within each class to preserve the overall class distribution of the data.

```{r}
set.seed(12345)
inTrain <- createDataPartition(y = training_data$classe, p = 0.8, list = FALSE)
training <- training_data[inTrain,]
testing <- training_data[-inTrain,]
```

The first model I fitted was a tree model, with pre-processing using principal component analysis and cross validation using the bootstrap method. The tree model has a relatively low accuracy of just 0.2845 on the testing set and an out of sample error of 2008.475.

```{r}
library(dplyr)
modelFit <- train(training$classe ~., method='rpart', preProcess = "pca", data = training)
confusionMatrix(testing$classe, predict(modelFit, testing))
compare_predictions <- data.frame(original=testing$classe, new_prediction=predict(modelFit, testing))
compare_predictions$class_prediction <- ifelse(compare_predictions$original == compare_predictions$new_prediction, "Correct", "Wrong")
wrong_predictions <- nrow(filter(compare_predictions, class_prediction == "Wrong"))
out_of_sample_error <- 1/(nrow(compare_predictions))*(wrong_predictions^2)
out_of_sample_error
```



The next model I have tried is a gradient boosting model, using 10-fold cross validation.

```{r}
library(dplyr)
modelFit <- train(training$classe ~., method='gbm', preProcess = "pca", trControl = trainControl(method = "cv"), data = training)
confusionMatrix(testing$classe, predict(modelFit, testing))
compare_predictions <- data.frame(original=testing$classe, new_prediction=predict(modelFit, testing))
compare_predictions$class_prediction <- ifelse(compare_predictions$original == compare_predictions$new_prediction, "Correct", "Wrong")
wrong_predictions <- nrow(filter(compare_predictions, class_prediction == "Wrong"))
out_of_sample_error <- 1/(nrow(compare_predictions))*(wrong_predictions^2)
out_of_sample_error
```


You can also embed plots, for example:

```{r, echo=FALSE}
plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

WHY IS THIS NOT WORKING???????    
